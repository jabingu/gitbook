{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction Copyright © jabingu.com 2019 all right reserved，powered by GitbookFile modification time： 2019-08-20 11:04:52 "},"conference/1-ICCD-2019.html":{"url":"conference/1-ICCD-2019.html","title":"ICCD 2019","keywords":"","body":"ICCD 2019 你好 你好 Copyright © jabingu.com 2019 all right reserved，powered by GitbookFile modification time： 2019-08-20 13:51:51 "},"conference/2-ICCD-2018.html":{"url":"conference/2-ICCD-2018.html","title":"ICCD 2018","keywords":"","body":"ICCD 2018 死亡是活过的生命，生活是在路上的死亡。 ——博尔赫斯　　 目录： Session 1: Best Papers SessionSession 2A: SSDSession 2B: Side ChannelsSession 3A: Security and CapabilitySession 3B: MicroarchitectureSession 4A: Logic and Circuit Design 1Session 4B: Design AutomationSession 5A: Novel ArchitecturesSession 5B: Memory 1Session 6A: Memory 2Session 6B: Logic and Circuit Design 2Session 7A: Accelerators and GPUsSession 7B: Potpouri 1Session 8A: NVMSession 8B: Test and VerificationSession 9A: Network on Chip and SynchronizationSession 9B: Potpouri 2Session 10A: File System and CloudSession 10B: FPGA and Machine Learning 1: Best Papers Session Composable Template Attacks Using Templates for Individual Architectural Components ​ Bozhi Liu, Roman Lysecky, Janet Meiling Wang Roveda With embedded systems and IoT devices being widely deployed nowadays, their security becomes a major concern. Among all possible attacks, side channel attacks (SCA) represent a major source of threats. For power side channels, template attacks have been proven to be efficient and widely applicable. Traditional template attacks require physical access to an identical target device for extensive profiling to construct the attack template. In this paper, we present a composable template attack that relaxes this requirement by constructing the attack template as a composition of templates from individual architectural components, including processor, caches, and memories. The proposed approach enables an attacker to construct a template using only information of a system's components and device models thereof. Thermal-Aware 3D Symmetrical Buffered Clock Tree Synthesis ​ Deok Keun Oh, Mu Jun Choi, Juho Kim The semiconductor industry has accepted three dimensional integrated circuits (3D ICs) as a possible solution to address speed and power management problems. In addition, 3D ICs have recently demonstrated a huge potential in reducing wire length and increasing the density of a chip. However, the growing density in chips such as TSV-based 3D ICs has brought increased temperature on chip and temperature gradients depending on location. Thus, through silicon via (TSV)-based 3D clock tree synthesis (CTS) causes thermal problem leading to large clock skew. We propose a novel 3D symmetrical buffered clock tree synthesis considering thermal variation. First, 3D abstract tree topology based on nearest neighbor selection with median cost (3D-NNM) is constructed by pairing sinks that have similar power consumption. Second, the layer assignment of internal nodes is determined for uniform TSV distribution. Third, in thermal-aware 3D deferred merging embedding (DME), the exact location of TSV is determined and wire routing/buffer insertion are performed after thermal profiles based on grid are obtained. The proposed method is verified using a 45nm process technology and utilized a predictive technology model (PTM) with HSPICE. Also, our CTS is evaluated for IBM and ISPD'09 benchmarks with no blockages. In experimental results, we can achieve average 18% of clock skew reduction compared to existing thermal-aware 3D CTS. Therefore, thermal-aware 3D symmetrical buffered clock tree synthesis presented in this work is very efficient for circuit reliability. Low-Overhead Microarchitectural Patching for Multicore Memory Subsystems ​ Doowon Lee, Opeoluwa Matthews, Valeria Bertacco In this work, we present μMemPatch, a comprehensive, efficient patching solution to overcome escaped design flaws in multicore memory subsystems at runtime. Unlike conventional microcode patching, μMemPatch strives to accurately pinpoint bug-prone microarchitectural states at runtime, by using a small programmable-logic fabric. μMemPatch comprises two main components: a bug-anticipation module and a bug-elusion module. The bug-anticipation module tracks, at runtime, the progress of microarchitectural events related to memory operations. Specifically, we model event sequences as finite state machines (FSM), where some of the FSM states represent bug-prone microarchitectural states. Upon detection of a bug-prone state, the bug-elusion module limits reorderings of instructions or memory accesses, so as to avoid falling into the bug state. We propose a few different bug-elusion methods, including squashing instructions, delaying cache evictions, and dynamically inserting fence operations. We implemented μMemPatch in a cycle-accurate full-system simulator. We then embedded eleven design bugs that span a wide range of bug types, which had been disclosed in product errata documents. Our evaluation with an in-house micro-benchmark suite and the SPLASH-2 suite shows that μMemPatch's bug-elusion methods successfully bypass all bugs at a performance impact of less than 1% on average (SPLASH-2). The area overhead in our setup is approximately 6% for an ARM Cortex-A9 core on average, over all bugs we considered. Power Grab in Aggressively Provisioned Data Centers: What is the Risk and What Can Be Done About It ​ Xiaofeng Hou, Luoyao Hao, Chao Li, Quan Chen, Wenli Zheng, Minyi Guo Aggressively provisioned data centers achieve great cost savings by over-committing the very expensive power distribution infrastructure. However, existing proposals for managing load power demand in such a data center are largely utilization-driven, overlooking power-related interferences among users. An important observation is that some tasks can impact existing power budget management framework and disrupt normal operation by taking away the precious public power capacity. This vulnerability exposes data centers to a new type of risk that we call power grab, which is essentially hostile power resource competition. It could worsen the performance-utilization tradeoff in a power-constrained computing environment. Anticipating a growing case for power-oriented competition, we propose CFP, a resilient power capacity management frame-work for improving the fairness and service quality in scale-out data centers. Our solution features a market-based power re-source allocation and billing scheme that involves users in the loop. It allows the data center to bypass the formidable task of identifying malicious users and defend against power grab with reward and punishment incentives. We build a proof-of-concept system and also evaluate our design with realistic Google cluster traces. Compared to prior arts, CFP can increase the average performance-cost ratio by 1.8X. It can boost the total throughput in an APDC by 15% under severe power contention. Our design allows scale-out data centers to safely exploit the benefits that power over-subscription may provide, with minor overhead. 数据中心；能耗调度 2A: SSD Pensieve: a Machine Learning Assisted SSD Layer for Extending the Lifetime ​ Te I, Murtuza Lokhandwala, Yu-Ching Hu, Hung-Wei Tseng As the capacity per unit cost dropping, flash-based SSDs become popular in various computing scenarios. However, the restricted program-erase cycles still severely limit cost-effectiveness of flash-based storage solutions. This paper proposes Pensieve, a machine-learning assisted SSD firmware layer that transparently helps reduce the demand for programs and erases. Pensieve efficiently classifies writing data into different compression categories without hints from software systems. Data with the same category may use a shared dictionary to compress the content, allowing Pensieve to further avoid duplications. As Pensieve does not require any modification in the software stack, Pensieve is compatible with existing applications, file systems and operating systems. With modern SSD architectures, implementing a Pensieve-compliant SSD also requires no additional hardware, providing a drop-in upgrade for existing storage systems. The experimental result on our prototype Pensieve SSD shows that Pensieve can reduce the amount of program operations by 19%, while delivering competitive performance. 机器学习；ssd；数据压缩 Selective Compression Scheme for Read Performance Improvement on Flash Devices ​ Qiao Li, Liang Shi, Riwei Pan, Cheng Ji, Xiaoqiang Li, Chun Jason Xue The increasing density and capacity of NAND flash memory leads to degraded reliability. To address the reliability issue, low-density parity-check code (LDPC) has been deployed in NAND flash memories due to its strong error correction capability. The drawback of LDPC is that, to correct data with high raw bit error rate (RBER), read latency will be amplified. To improve read performance, this paper proposes to apply lossless compression to reduce RBER on data pages. However, compression and decompression incur time overheads. Compressing all the data pages for RBER reduction will degrade write performance. In addition, the variation of compression ratio leads to variation of RBER reduction, thus varied read latency reduction. In this work, a selective data compression scheme is proposed for read performance improvement. Both read frequency and compression ratio of data are taken into consideration. Data in a flash page with high read frequency and good compressibility are prioritized for compression. Experimental results show that the proposed scheme can improve read performance by 42% on average, without impacting write performance. 数据压缩；LDPC OSPADA: One-Shot Programming Aware Data Allocation Policy to Improve 3D NAND Flash Read Performance Fei Wu, Zuo Lu, You Zhou, Xubin He, Zhi-hu Tan, Changsheng Xie: Charge trap (CT) based 3D NAND flash is predominating the flash storage market due to higher density, better performance and endurance than planar flash. CT-based 3D flash programs multiple pages in a word line at a time, called one-shot programming, unlike planar flash which programs one page at a time. Solid state drives (SSDs) utilize the internal parallelism to improve the performance, but one-shot programming is likely to program logically sequential data into one parallel unit (i.e., a plane) and thus degrades the read parallelism. In this paper, we propose a one-shot programming aware data allocation policy, called OSPADA, to improve the read performance of CT flash based SSDs by enhancing read parallelism. OSPADA reorders written data to distribute logically sequential data into different parallel units using the distance aware round-robin strategy. Experimental results show that OSPADA improves the read performance by up to 22.8% compared with traditional dynamic data allocation policies. 由于读写单元和擦除的单元的大小问题 Cap: Exploiting Data Correlations to Improve the Performance and Endurance of SSD RAID ​ Gaoxiang Xu, Zhipeng Tan, Dan Feng, Yifeng Zhu, Xinyan Zhang, Jie Xu Parity-based RAID provides system-level fault tolerance. However, parity updates caused by small writes introduce lots of extra I/Os, degrading I/O performance and wearing SSDs out. It has been proposed to use Non-Volatile Memory (NVM) as a parity cache on an SSD RAID to postpone parity updates until the whole stripe has been updated. However, this often fails because of skewed distribution of hot data chunks within a stripe. In real workloads, it is often difficult to achieve a full-stripe update even after a long delay. In this paper, we propose a Correlation aware parity caching scheme, called Cap, for SSD-based RAIDs. The key idea behind Cap is to periodically reconstruct correlated hot data chunks into a new stripe. Since these data chunks have a strong correlation, they tend to be updated together within a short time span. This co-update within a stripe more efficiently utilizes the parity cache to convert partial-stripe updates into a full-stripe update. We have implemented Cap on a RAID-5 SSD array in Linux Kernel 4.3. Experimental results show that Cap improves the I/O bandwidth by 54%~145% compared with the Linux software RAID. Compared with the state-of-the-art parity caching scheme PPC, Cap improves the I/O bandwidth by 14%~31%. RAID;data chunk； 2B: Side Channels 3A: Security and Capability CheriRTOS: A Capability Model for Embedded Devices ​ Hongyan Xia, Jonathan Woodruff, Hadrien Barral, Lawrence Esswood, Alexandre Joannou, Robert Kovacsics, David Chisnall, Michael Roe, Brooks Davis, Edward Napierala, John Baldwin, Khilan Gudka, Peter G. Neumann, Alexander Richardson, Simon W. Moore, Robert N. M. Watson: Embedded systems are deployed ubiquitously among various sectors including automotive, medical, robotics and avionics. As these devices become increasingly connected, the attack surface also increases tremendously; new mechanisms must be deployed to defend against more sophisticated attacks while not violating resource constraints. In this paper we present CheriRTOS on CHERI-64, a hardware-software platform atop Capability Hardware Enhanced RISC Instructions (CHERI) for embedded systems. Our system provides efficient and scalable task isolation, fast and secure inter-task communication, fine-grained memory safety, and real-time guarantees, using hardware capabilities as the sole protection mechanism. We summarize state-of-the-art security and memory safety for embedded systems for comparison with our platform, illustrating the superior substrate provided by CHERI's capabilities. Finally, our evaluations show that a capability system can be implemented within the constraints of embedded systems. 嵌入式设备 ReadPRO: Read Prioritization Scheduling in ORAM for Efficient Obfuscation in Main Memories ​ Joydeep Rakshit, Kartik Mohanram: Modern memory systems are susceptible to data confidentiality attacks that leverage memory access pattern information to obtain secret data. Oblivious RAM (ORAM) is a secure cryptographic construct that effectively thwarts access-pattern-based attacks. However, in Path ORAM (state-of-the-art efficient ORAM for main memories) and its variants, each memory request (read or write) is transformed to an ORAM access, which is a sequence of read and write operations, increasing the latency of the memory requests and degrading system performance. In practice, the ORAM access for a read request is on the critical path of program execution, blocked by ORAM accesses for older write requests. Although modern memory controllers (MCs) realize read prioritization through write buffering, the ORAM access translation of each memory request to multiple memory read and write operations results in frequent MC write buffer overflow, decreasing its efficiency. ReadPRO (Read Prioritization) scheduling in ORAM addresses this challenge by promoting read requests over write requests in the ORAM controller prior to their ORAM access translation, while preserving all data dependencies. ReadPRO complements read promotion with staggered writes, wherein ORAM accesses for write requests can be paused securely to serve ORAM accesses for read requests. Full-system evaluations on composite SPEC CPU2006 workloads show that ReadPRO decreases the average ORAM read latency by 75%, improving system performance by 40%. 内存攻击 SGXlinger: A New Side-Channel Attack Vector Based on Interrupt Latency Against Enclave Execution. ​ Wenjian He, Wei Zhang, Sanjeev Das, Yang Liu Software Guard Extension (SGX) is a new security feature that has been released in recent Intel commodity processors. It is designed to provide a user program with a strongly shielded environment against other components in the system, including the OS, firmware and hardware peripherals. With SGX, developers can securely deploy critical applications on untrusted remote platforms without the concern of information leakage. However, researchers have found several attacks against SGX, suggesting blind reliance on SGX is inadvisable, and promoting the need for a comprehensive study on the security property of SGX. In this paper, we discover a new attack vector SGXlinger to disclose information inside the protected program. Our attack monitors the interrupt latency of the SGX-protected program, and it is the first time that the interrupt latency is leveraged as a side-channel. We develop a framework to repeatedly measure the interrupt latency of an enclave program, and the evaluation shows we can learn coarse-grained information inside the shielded environment. In an experimental setting, we measure that the information leakage rate of the proposed side-channel can reach up to 35 Kbps. Breaking the Oblivious-RAM Bandwidth Wall. Hamza Omar, Syed Kamran Haider, Ling Ren, Marten van Dijk, Omer Khan PathORAM is a popular security primitive for obfuscating memory access patterns from a secure processor to an insecure main memory. Emerging throughput multicore and GPU processors provide immense memory bandwidth via multiple on-chip memory controllers. PathORAM translates a single off-chip cache line access into ~100 cache lines, thereby stressing the available memory bandwidth. However, current PathORAM scheme shows degradation of bandwidth utilization with an increase in the number of memory controllers. This deprivation in bandwidth utilization is primarily due to the fact that PathORAM falls short in proportionate distribution of memory accesses among all available on-chip memory controllers. This paper presents a novel ORAM path distribution scheme that ensures balanced load distribution among parallel on-chip memory controllers, and consequently improves secure processor performance by ~24% over state-of-the-art PathORAM scheme. 3B: Microarchitecture 4A: Logic and Circuit Design 1 4B: Design Automation 5A: Novel Architectures 5B: Memory 1 CART: Cache Access Reordering Tree for Efficient Cache and Memory Accesses in GPUs. ​ Yongbin Gu, Lizhong Chen Graphics processing units (GPUs) have been increasingly used to accelerate general purpose computing. Thousands of concurrently running threads in a GPU demand a highly efficient memory subsystem for data supply. A key factor that affects the memory subsystem is the order of memory accesses. While reordering memory accesses at L2 cache has large potential benefits to both cache and DRAM, little work has been conducted to exploit this. In this paper, we investigate the largely unexplored opportunity of L2 cache access reordering. We propose Cache Access Reordering Tree (CART), a novel architecture that can improve memory subsystem efficiency by actively reordering memory accesses at L2 cache to be cache-friendly and DRAM-friendly. Evaluation results using a wide range of benchmarks show that, the proposed CART is able to improve the average IPC of memory intensive benchmarks by 34.2% with only 1.7% area overhead. 在cache中进行访问模式的修改 ArchSampler: Architecture-Aware Memory Sampling Library for In-Memory Applications. Jian Zhou, Jun Wang: With the explosive rate of data growth, the limited scalability of the DRAM technology defies the performance potentials for in-memory applications. Fortunately, emerging non-volatile memory (NVM) technologies, such as Phase-Change Memory (PCM) and Memristor, are promising candidates for replacing DRAM. Emerging NVMs are very dense, hence promise large capacities. Additionally, NVMs are non-volatile, thus enable persistent applications and byte-addressable files. Both density and persistency are key enablers for in-memory applications. On the other side, emerging NVMs are slower than DRAM, thus optimizing for locality and avoiding contentions are key aspects to unlock the NVM performance. In this paper, we study the impact of memory contentions and architecture-oblivious implementations on the performance of sampling based in-memory approximation. Sampling has become an imperative technique used to accelerate big data processing, especially in today's emerging in-memory computing. However, we observe multiple times slow-down for nave and default implementations of in-memory data sampling. Accordingly, we propose ArchSampler, an architecture-aware sampling library. The main idea is to exploits the free choice of data samples to dynamically select which bank as a host to serve memory requests. Hence, ArchSampler enables efficient and high performing sampling through employing its knowledge of the NVM architectural details to maximize data locality and avoiding interthread contentions. Our evaluation shows that ArchSampler can achieve up to 1.62 speed up (1.20 on average) for different in-memory applications. In-memory PIM-TGAN: A Processing-in-Memory Accelerator for Ternary Generative Adversarial Networks. ​ Adnan Siraj Rakin, Shaahin Angizi, Zhezhi He, Deliang Fan Generative Adversarial Network (GAN) has emerged as one of the most promising semi-supervised learning methods where two neural nets train themselves in a competitive environment. In this paper, as far as we know, we are the first to present a statistically trained Ternarized Generative Adversarial Network (TGAN) with fully ternarized weights (i.e. -1,0,+1) to massively reduce the need for computation and storage resources in the conventional GAN structures. In the proposed TGAN, the computationally expensive convolution operations (i.e. Multiplication and Accumulation) in both generator and discriminator's forward path are converted into hardware-friendly Addition/Subtraction operations. Accordingly, we propose a Processing-in-Memory accelerator for TGAN called (PIM-TGAN) based on Spin-Orbit Torque Magnetic Random Access Memory (SOT-MRAM) computational sub-arrays to efficiently accelerate the training process of GAN within non-volatile memory. In addition, we propose a parallelism technique to further enhance the training efficiency of TGAN. Our device-to-architecture co-simulation results show that, with almost the same inception score to the baseline GAN with floating point number weights on different data-sets, the proposed PIM-TGAN can obtain ~25.6× better energy-efficiency and 22× speedup compared to GPU platform averagely, and, 9.2× better energy-efficiency and 5.4× speedup over the best processing-in-ReRAM accelerators. PIM加速GAN网络 Path Prefetching: Accelerating Index Searches for In-Memory Databases. ​ Shuo Li, Zhiguang Chen, Nong Xiao, Guangyu Sun In-memory databases (IMDBs) store all working data in main memory, which makes memory accesses become the dominant factor of the whole system performance. Micro-architectural studies of mainstream in-memory on-line transaction processing (OLTP) systems show that more than half of the execution time goes to memory stalls. Moreover, for IMDBs that adopt aggressive transaction compilation optimizations, data misses from the last-level cache (LLC) are responsible for the majority of the overall stall time. In this paper, through profiling analysis of IMDBs we observe that index access misses dominate LLC data misses. Based on the key observation that adjacent keys tend to follow similar traversal paths in ordered index searches, we propose the path prefetching to mitigate LLC misses induced by ordered index searches, which records mappings between keys and their traversal paths and then generate prefetches for future same/adjacent keys. Experimental results show that for ordered index searches the proposed path prefetcher provides an average speedup of 27.4% over the baseline with no prefetching. Reducing Inter-Application Interferences in Integrated CPU-GPU Heterogeneous Architecture. ​ Hao Wen, Wei Zhang Current heterogeneous CPU-GPU architectures integrate general purpose CPUs and highly thread-level parallelized GPUs (Graphic Processing Units) in the same die. The contention in shared resources between CPU and GPU, such as the last level cache (LLC), interconnection network and DRAM, may degrade both CPU and GPU performance. Our experimental results show that GPU applications tend to have much more power than CPU applications to compete for the shared resources in LLC and on-chip network, and therefore make CPU suffer from more performance loss. To reduce the GPU's negative impact on CPU performance, we propose a simple yet effective method based on probability to control the LLC replacement policy for reducing the CPU's inter-core conflict misses caused by GPU without significantly impacting GPU performance. In addition, we develop two strategies to combine the probability based method for the LLC and an existing technique called virtual channel partition (VCP) for the interconnection network to further improve the CPU performance. The first strategy statically uses an empirically pre-determined probability value associated with VCP, which can improve the CPU performance by 26% on average, but degrades GPU performance by 5%. The second strategy uses a sampling method to monitor the network congestion and dynamically adjust the probability value used, which can improve the CPU performance by 24%, and only have 1 or 2% performance overhead on GPU applications. 6A: Memory 2 Solar-DRAM: Reducing DRAM Access Latency by Exploiting the Variation in Local Bitlines. ​ Jeremie Kim, Minesh Patel, Hasan Hassan, Onur Mutlu: DRAM latency is a major bottleneck for many applications in modern computing systems. In this work, we rigorously characterize the effects of reducing DRAM access latency on 282 state-of-the-art LPDDR4 DRAM modules. As found in prior work on older DRAM generations (DDR3), we show that regions of LPDDR4 DRAM modules can be accessed with latencies that are significantly lower than manufacturer-specified values without causing failures. We present novel data that 1) further supports the viability of such latency reduction mechanisms and 2) exposes a variety of new cases in which access latencies can be effectively reduced. Using our observations, we propose a new low-cost mechanism, Solar-DRAM, that 1) identifies failure-prone regions of DRAM at reduced latency and 2) robustly reduces average DRAM access latency while maintaining data correctness, by issuing DRAM requests with reduced access latencies to non-failure-prone DRAM regions. We evaluate Solar-DRAM on a wide variety of multi-core workloads and show that for 4-core homogeneous workloads, Solar-DRAM provides an average (maximum) system performance improvement of 4.31% (10.87%) compared to using the default fixed DRAM access latency. Scalable and Efficient Virtual Memory Sharing in Heterogeneous SoCs with TLB Prefetching and MMU-Aware DMA Engine. Andreas Kurth, Pirmin Vogel, Andrea Marongiu, Luca Benini: Shared virtual memory (SVM) is key in heterogeneous systems on chip (SoCs), which combine a general-purpose host processor with a many-core accelerator, both for programmability and to avoid data duplication. However, SVM can bring a significant run time overhead when translation lookaside buffer (TLB) entries are missing. Moreover, allowing DMA burst transfers to write SVM traditionally requires buffers to absorb transfers that miss in the TLB. These buffers have to be overprovisioned for the maximum burst size, wasting precious on-chip memory, and stall all SVM accesses once they are full, hampering the scalability of parallel accelerators. In this work, we present our SVM solution that avoids the majority of TLB misses with prefetching, supports parallel burst DMA transfers without additional buffers, and can be scaled with the workload and number of parallel processors. Our solution is based on three novel concepts: To minimize the rate of TLB misses, the TLB is proactively filled by compiler-generated Prefetching Helper Threads, which use run-time information to issue timely prefetches. To reduce the latency of TLB misses, misses are handled by a variable number of parallel Miss Handling Helper Threads. To support parallel burst DMA transfers to SVM without additional buffers, we add lightweight hardware to a standard DMA engine to detect and react to TLB misses. Compared to the state of the art, our work improves accelerator performance for memory-intensive kernels by up to 4~ and by up to 60% for irregular and regular memory access patterns, respectively. DR DRAM: Accelerating Memory-Read-Intensive Applications. ​ Yuhai Cao, Chao Li, Quan Chen, Jingwen Leng, Minyi Guo, Jing Wang, Weigong Zhang: Today, many data analytic workloads such as graph processing and neural network desire efficient memory read operation. The need for preprocessing various raw data also demands enhanced memory read bandwidth. Unfortunately, due to the necessity of dynamic refresh, modern DRAM system has to stall memory access during each refresh cycle. As DRAM device density continues to grow, the refresh time also needs to extend to cover more memory rows. Consequently, DRAM refresh operation can be a crucial throughput bottleneck for memory read intensive (MRI) data processing tasks. To fully unleash the performance of these applications, we revisit conventional DRAM architecture and refresh mechanism. We propose DR DRAM, an application-specific memory design approach that makes a novel tradeoff between read and write performance. Simply put, DR has two layers of meaning: device refresh and data recovery. It aims at eliminating stall by enabling read and refresh operations to be done simultaneously. Unlike traditional schemes, DR explores device refresh that only refreshes a specific device at a time. Meanwhile, DR increases read efficiency by recovering the inaccessible data that resides on a device under refreshing. Our design can be implemented on existing redundant data storage area on DRAM. In this paper we detail DR's architecture and protocol design. We evaluate it on a cycle accurate simulator. Our results show that DR can nearly eliminate refresh overhead for memory read operation and brings up to 12% extra maximum read bandwidth and 50~60% latency improvement on present DRR4 device. Puzzle Memory: Multifractional Partitioned Heterogeneous Memory Scheme. ​ Jee Ho Ryoo, Shuang Song, Lizy K. John As current main memory technology scaling is coming close to an end due to its physical limitations, many emerging memory technologies are coming to the market to fill the scaling gap. Future memory systems will require a heterogeneous memory architecture where one technology acts as a low latency memory whereas the other acts as a high capacity memory. This will allow the future main memory system to continue to scale in terms of capacity, yet have similar or slightly better latency than today's DRAM technology. Prior work on data management in heterogeneous memory has optimized one or a maximum of two components in the computing stack. However, different components are good at different tasks in data management, so in the era of heterogeneous memory, it is inevitable that cooperative multi-component data management will be adopted in future systems. We propose a heterogeneous memory layout where two memories are laid out asymmetrically. The operating system is aware of this layout and places pages with different locality characteristics in different regions of memory. Finally, a custom hardware performs the data remapping to optimize the data placement at finer granularity than what is visible to the operating system. In the end, we show that our multi-component cooperative data management scheme can improve the overall system performance by up to 40%. 6B: Logic and Circuit Design 2 7A: Accelerators and GPUs 7B: Potpouri 1 8A: NVM Breeze: User-Level Access to Non-Volatile Main Memories for Legacy Software ​ Amirsaman Memaripour, Steven Swanson: Non-volatile main memory (NVMM) technologies, such as phase change memory and 3D XPoint, offer DRAM-like performance and byte-addressable access to persistent data. A wide range of applications (e.g., key-value stores and database systems) stand to benefit from the performance potential of these technologies. These potential benefits are greatest when applications can access memory directly via load/store instructions rather than conventional file-based interfaces. This approach presents several challenges. In particular, applications need guaranteed consistency and safety semantics to protect their data structures in the face of system failures and programming errors. Implementing data structures that meet these requirements is challenging and error-prone. Researchers have proposed several libraries and programming language extensions that simplify this task, but, to date, all the proposed solutions either require pervasive changes to existing software or rely on special hardware support. As a result, porting legacy applications to leverage NVMM is likely to be prohibitively difficult and time consuming. We propose Breeze, a NVMM toolchain that minimizes the changes necessary to enable legacy code to reap the benefits of directly accessing NVMM. Breeze guarantees data consistency and validity of persistent pointers regardless of failures. The toolchain transparently detects and logs writes to NVMM and provides a simple mechanism for identifying atomic sections while avoiding complications common in previous systems such as special persistent pointer types. Porting Memcached and MongoDB to use Breeze only requires changes to 5% of the source code compared to 7-14% for NVML and NVM-Direct. Breeze also provides equal or superior performance compared to NVML and NVM-Direct, outperforming them by up to 10x. R-Cache: A Highly Set-Associative In-Package Cache Using Memristive Arrays. ​ Payman Behnam, Arjun Pal Chowdhury, Mahdi Nazm Bojnordi: Over the past decade, three-dimensional die stacking technology has been considered for building large-scale in-package memory systems. In particular, in-package DRAM cache has been considered as a promising solution for high band-width and large-scale cache architectures. There are, however, significant challenges such as limited energy efficiency, costly tag management, and physical limitations for scalability that need to be effectively addressed before one can adopt in-package caches in the real-world applications. This paper proposes R-Cache, an in-package cache made by 3D die stacking of memristive memory arrays to alleviate the above mentioned challenges. Our simulation results on a set of memory intensive parallel applications indicate that R-Cache outperforms the state-of-the-art proposals for in-package caches. R-Cache improves performance by 38% and 27% over the state-of-the-art direct mapped and set associative cache architectures, respectively. Moreover, R-Cache results in averages of 40% and 27% energy reductions as compared to the direct mapped and set-associative cache systems. A Highly Non-Volatile Memory Scalable and Efficient File System. ​ Fan Yang, Junbin Kang, Shuai Ma, Jinpeng Huai: With the rapid development of fast and byte-addressable non-volatile memories (NVMs), hybrid NVM/DRAM storage systems become promising for computer systems. Existing NVM file systems have already been optimized around the NVM properties. However, they inherit some design choices of block-oriented storage devices that lead to scalability bottlenecks and data copy overhead for ensuring data consistency. In this paper, we present noseFS, a highly non-volatile memory scalable and efficient File System. It is designed to achieve high performance through a bundle of novel techniques: (1) a scalable lightweight naming integrating VFS with the underlying file system namespace, (2) a fine-grained byte-unit file index tree avoiding redundant copy overhead introduced by Copy-On-Write, (3) a lightweight journaling providing atomicity and scalability on many-core platforms, and (4) a lightweight atomic-mmap providing strong consistency guarantee with low overhead by tracking dirty pages. Experimental results show that noseFS performs much better than the state-of-the-art file systems with equally strong data consistency guarantees, and achieves near-linear scalability on a 40-core machine. NVCool: When Non-Volatile Caches Meet Cold Boot Attacks. ​ Xiang Pan, Anys Bacha, Spencer Rudolph, Li Zhou, Yinqian Zhang, Radu Teodorescu: Non-volatile memories (NVMs) are expected to replace traditional DRAM and SRAM for both off-chip and on-chip storage. It is therefore crucial to understand their security vulnerabilities before they are deployed widely. This paper shows that NVM caches are vulnerable to so-called \"cold boot\" attacks, which involve physical access to the processor's cache. SRAM caches have generally been assumed invulnerable to cold boot attacks, because SRAM data is only persistent for a few milliseconds even at cold temperatures. Our study explores cold boot attacks on NVM caches and defenses against them. In particular, this paper demonstrates that hard disk encryption keys can be extracted from the NVM cache in multiple attack scenarios. We demonstrate a reproducible attack with very high probability of success. This paper also proposes an effective software-based countermeasure that can completely eliminate the vulnerability of NVM caches to cold boot attacks with a reasonable performance overhead. 8B: Test and Verification 9A: Network on Chip and Synchronization 9B: Potpouri 2 10A: File System and Cloud 10B: FPGA and Machine Learning Copyright © jabingu.com 2019 all right reserved，powered by GitbookFile modification time： 2019-08-20 14:55:41 "},"conference/3-ICCD-2017.html":{"url":"conference/3-ICCD-2017.html","title":"ICCD 2017","keywords":"","body":"ICCD 2017 死亡是活过的生命，生活是在路上的死亡。 ——博尔赫斯　　 目录： **Best Papers Session Session 1A: Hardware Security I Session 1B: Read-Write Optimizations for Non-Volatile Memory Session 2A: Stochastic, Approximate, and Unary Computing Session 2B: Energy-Efficiency through Heterogeneity Session 3A: Debugging and Validation Session 3B: Graph Processing and NoC Architectures Session 4A: EDA with Focus on Multicore, FPGAs, and 3D Session 4B: Hardware Acceleration for Neural Networks Session 5A: Hardware Security II Session 5B: Memory and Cache Optimizations Session 6A: Verification and Fault Tolerance Session 6B: Lithography and Patterning Special Session 1: On How to Design and Manage Complex Heterogeneous Distributed Computing Systems Session 7A: LCD with Focus on Emerging Technology Session 7B: Power-Performance Optimization of Multicore Architecture Session 8A: Synthesis and Security Session 8B: Cloud and Storage Solutions Special Session 2: Effective Voltage Scaling in Late CMOS Era Special Session 3: Spin-Computing: Lower the Barrier between Memory and Logic Session 9A: Architecture and Microarchitecture Optimizations Session 9B: Novel Architecture with 3D and Flash Memory Best Papers Session S1A: Hardware Security I S1B: Read-Write Optimizations for Non-Volatile Memory Adaptive Prefetching for Accelerating Read and Write in NVM-Based File Systems ​ Shengan Zheng, Hong Mei, Linpeng Huang, Yanyan Shen, Yanmin Zhu: The byte-addressable Non-Volatile Memory (NVM) offers fast, fine-grained access to persistent storage. While DRAM and NVM have similar read performance, the write operations of existing NVM materials incur longer latency and lower bandwidth than DRAM. This read-write asymmetry nature of NVM causes two bottlenecks for accessing read-and write-intensive file data: expensive data block lookups via file inner structure and high-latency direct writes to data blocks in NVM. However, existing NVM-based file systems fail to address both bottlenecks well. This paper presents WARP, an adaptive prefetching module designed for NVM-based file systems, which aims to deal with two bottlenecks effectively. WARP employs two acceleration approaches: 1) mapping data blocks into kernel virtual address space to bypass the indirection of file inner structure for read-intensive file data; and 2) allocating DRAM buffer to absorb frequent writes for write-intensive file data. We design a WARP benefit model to identify read-and write-intensive access patterns for file data, and use a successor prediction model to predict future data access based on historical file access traces. With WARP, we are able to prefetch file data according to both file access patterns and traces with consistency guarantee. WARP can be implemented on various NVM-based file systems, and we choose HMVFS for the experiments. The evaluation results show that HMVFS with WARP provides high prefetching accuracy and up to 32%-83% improvement compared with the state-of-the-art NVM-based file systems. NVM-based file system；预取；buffer A Cost-Efficient NVM-Based Journaling Scheme for File Systems. ​ Xiaoyi Zhang, Dan Feng, Yu Hua, Jianxi Chen Modern file systems employ journaling techniques to guarantee data consistency in case of unexpected system crashes or power failures. However, journaling file systems usually suffer from performance decrease due to the extra journal writes. Moreover, the emerging non-volatile memory technologies (NVMs) have the potential capability to improve the performance of journaling file systems by being deployed as the journaling storage devices. However, traditional journaling techniques, which are designed for hard disks, fail to perform efficiently in NVMs. In order to address this problem, we propose an NVM-based journaling scheme, called NJS. The basic idea behind NJS is to reduce the journaling overhead of traditional file systems while fully exploiting the byte-accessibility characteristic, and alleviating the relatively slow write and endurance limitation of NVM. Our NJS consists of three major contributions: (i) In order to minimize the amount of journal writes, NJS only needs to write the metadata of file systems and over-write data to NVM as write-ahead logging, thus alleviating the relatively slow write and endurance limitation of NVM. (ii) We propose a novel journaling update scheme in which the journaling data blocks can be updated in the byte-granularity based on the difference of the old and new versions of journal blocks, thus fully exploiting the unique byte-accessibility characteristic of NVM. (iii) NJS includes a garbage collection mechanism that absorbs the redundant journal updates, and actively delays the checkpointing to the file system. Evaluation results show the efficiency and efficacy of NJS. For example, compared with original Ext4 with a ramdisk-based journaling device, the throughput improvement of Ext4 with our NJS is up to 137.1%. NVM-Based Journaling Scheme； TDV Cache: Organizing Off-Chip DRAM Cache of NVMM from a Fusion Perspective. ​ Tianyue Lu, Yuhang Liu, Haiyang Pan, Mingyu Chen Emerging Non-Volatile Memory (NVM) provides both larger memory capacity and higher energy efficiency, but has much longer access latency than traditional DRAM, thus DRAM can be used as an efficient cache to hide the long latency of Non-Volatile Main Memory (NVMM) system. Transparent Off-chip DRAM cache (TOD cache) is a new DRAM cache structure where off-chip DRAM module is used as L4 cache and managed by hardware. The capacity and latency ratio of TOD cache over NVM are both quite different from those of traditional on-chip SRAM or die-stacked DRAM cache over off-chip DRAM memory. All the factors including hit latency, miss latency and hit rate need to be re-considered for TOD cache design. In this study, we first point out that three types of traditional cache schemes cannot be used directly for TOD cache, since set-associative cache suffers from extra tag lookup latency, direct-mapped cache has low hit rate and tag cache is too small to efficiently hold the working sets of tags for DRAM cache. Based on these observations, we propose a novel cache scheme, TDV, that fuses these three different types of cache together to take their advantages. In TDV, a direct-mapped cache is used as the first-level cache to achieve short access latency, a set-associative victim cache is taken as the second-level cache to obtain extra high hit rate, and a SRAM tag cache only serves for the victim cache rather than the whole DRAM cache and thus improves the hit rate of tag cache significantly. The simulation results show that, TDV cache has a performance improvement of 6.3% and 8.3% on average than state-of-the-art direct-mapped (Alloy cache) and set-associative cache (ATCache) with same DRAM and SRAM capacity. L4 cache RCTP: Region Correlated Temporal Prefetcher Dennis Antony Varkey, Biswabandan Panda, Madhu Mutyam: Hardware prefetcher is an essential component of modern processors that helps in boosting system performance by fetching the data before processor demands for the same. Hardware prefetching techniques have been proposed to exploit various kinds of access patterns. However, there are applications that are highly irregular in nature that evolved in the past decade, and have massive memory footprint. Temporal prefetching techniques are effective in predicting the future addresses of these irregular applications. Prior works on temporal prefetching use large data structures to store temporal patterns and future memory accesses are predicted using these patterns. However, these techniques predict future accesses only when there is a pattern that is already trained for a given cache line address. To address this issue, we propose Region Correlated Temporal Prefetcher (RCTP). Our technique correlates temporal patterns of memory regions and predicts future accesses for the region whose patterns are yet to be populated. Thus, RCTP helps in predicting cache line addresses whose first access is yet to happen unlike the traditional temporal prefetchers. We evaluate RCTP on SPEC CPU 2006, CRONO, and PBBS benchmark suites. RCTP outperforms the state-of-the-art temporal prefetcher named ISB by 26%, and a recent delta prefetcher called VLDP by 6%. This improvement comes with a hardware overhead of 1KB over ISB; however, RCTP does not require off-chip storage, unlike other temporal prefetchers. 硬件预取；时间预取器； A Shingle-Aware Persistent Cache Management Scheme for DM-SMR Disks. ​ Tianming Yang, Haitao Wu, Ping Huang, Fei Zhang: Shingled Magnetic Recording (SMR) Technology is a new promising disk technology, which enables high areal density for disk storage by adopting writing tracks in an overlapped manner. Due to track overlapping, however, SMR devices cannot support in-place updates on shingled disk tracks, as overwriting shingled tracks would damage previously written data on neighboring tracks, resulting in poor random write performance. Drive-managed SMR (DM-SMR) devices attempt to circumvent this idiosyncrasy by deploying an internal persistent cache which absorbs incoming writes temporally and persists them later on in batches, while providing backward compatibility by reserving the same block access interface. The persistent cache management policy has to maintain the mapping associations between fresh writes in the persistent cache and their target destinations on the disk, resulting in a so-called shingle translation layer (STL). Leveraging the sequential-only write property of shingled devices, in this paper, we propose a new shingle aware persistent cache cleaning policy for DM-SMR drives. Unlike traditional management polices, our new policy first merges cached updates by flushing writes that can be safely written to the disk, i.e., in the shingle direction, so that the cache space can be freed without paying the cost of read-modify-write. It then defaults to the normal cache merging process if it needs to reclaim more cache space. Our evaluations have shown that our persistent cache management policy delivers better performance (by up to 2.5X) via dramatically reducing write amplification associated with persistent cache cleaning and alleviating fragmented reads. 磁盘SMR S2A: Stochastic, Approximate, and Unary Computing S2B: Energy-Efficiency through Heterogeneity S3A: Debugging and Validation S3B: Graph Processing and NoC Architectures S4A: EDA with Focus on Multicore, FPGAs, and 3D S4B: Hardware Acceleration for Neural Networks S5A: Hardware Security II S5B: Memory and Cache Optimizations DAAIP: Deadblock Aware Adaptive Insertion Policy for High Performance Caching. ​ Newton, Sujit Kr Mahto, Suhit Pai, Virendra Singh: The commonly used LRU replacement policy for management of shared last-level cache (LLC) is not efficient as the policy is sharing-oblivious. LRU policy is suitable for applications which show a high-degree of data locality i.e. applications which are cache-friendly. However, applications with working data set greater than the available cache size or poor temporal locality perform poorly with LRU as most of the cache lines inserted by them simply traverse from MRU to LRU position without being re-referenced. Such applications are streaming in their cache behaviour and have very less data reuse. LRU policy makes inefficient use of shared caches for application mixes which are a combination of cache-friendly and streaming applications as the policy treats each cache line independently and doesn't learn from application's past cache reuse behaviour. We show that simple adaptive changes to the insertion policy can significantly improve system's performance. We propose Deadblock Aware Adaptive Insertion Policy (DAAIP) which dynamically adapts to the changing cache behaviour of applications sharing the LLC. DAAIP protects the data of application having high temporal locality from high access rate thrashing/streaming applications. Our proposed mechanism monitors each application at-runtime using cost-effective hardware circuits. The information collected is used to dynamically modify the insertion policy and implicitly partition the cache in favour of application showing more data locality. Our evaluation, with 39 multiprogrammed workloads, shows that DAAIP improves performance of dual-core system by up to 21% and on an average 5.8% over LLC caches managed by SRRIP replacement policy. We show that DAAIP also outperforms state-of-the-art cache replacement policy ABRip by 4.6% on system throughput metric. cache替换策略 Dual Dictionary Compression for the Last Level Cache. ​ Akshay Lahiry, David R. Kaeli: The performance of GPUs is rapidly improving as the top GPU vendors keep pushing the boundaries of process technologies. While larger die sizes help improve performance given the nature of parallel workloads, additional architectural improvements can also help by utilizing the available die real estate more efficiently. Introducing a compressed Last Level Cache (LLC) can make better use of die area, and can improve memory system performance. With widespread adoption of high-resolution displays, most modern game developers are trying to generate high quality graphics output leveraging state-of-the-art GPUs, all of which greatly increases amount of data that needs to be processed. These modern graphics workloads will need to rely on compression to help save memory bandwidth and improve the performance of the LLC. A compressed LLC can help by increasing the hit-rate due to logical cache expansion, as well as provide bandwidth savings due to compressed data on the memory bus. In this paper we propose a novel scheme to extend dynamic dictionary-based compression to store compressed data in memory. Current dictionary-based compression schemes need to decompress the data when a cache block gets evicted. This is because the dynamic dictionary entries are not guaranteed to stay the same and data consistency cannot be maintained. This results in bandwidth savings that is limited to the logical cache expansion. We propose a dual-dictionary scheme (DDC) that can help maintain data consistency, as well as improve bandwidth savings. Our scheme saves bandwidth by coupling logical cache expansion with compressed data on the memory bus. We achieve bandwidth savings of 18.55% for reads and 11.01% for writes, on average, for a diverse range of graphics workloads. compression Jenga: Efficient Fault Tolerance for Stacked DRAM ​ Georgios Mappouras, Alireza Vahid, A. Robert Calderbank, Derek R. Hower, Daniel J. Sorin: In this paper, we introduce Jenga, a new scheme for protecting 3D DRAM, specifically high bandwidth memory (HBM), from failures in bits, rows, banks, channels, dies, and TSVs. By providing redundancy at the granularity of a cache block-rather than across blocks, as in the current state of the art-Jenga achieves greater error-free performance and lower error recovery latency. We show that Jenga's runtime is on average only 1.03x the runtime of our Baseline across a range of benchmarks. Additionally, for memory intensive benchmarks, Jenga is on average 1.11x faster than prior work. SelSMaP: A Selective Stride Masking Prefetching Scheme. Jiajun Wang, Reena Panda, Lizy Kurian John: Although prefetching concepts have been proposed for decades, new challenges are introduced by sophisticated system architecture and emerging applications. Large instruction windows coupled with out-of-order execution makes program data access sequence distorted from cache perspective. Big data applications stress memory subsystems heavily with their large working set sizes and complex data access patterns. To address such challenges, this work proposes a high performance hardware prefetching scheme, SelSMaP. SelSMaP is able to detect both regular and non-uniform stride patterns by taking the minimum observed address offset (called a reference stride) as a heuristic. We evaluated SelSMaP with CloudSuite workloads and SPEC CPU2006 benchmarks. SelSMaP achieves an average CloudSuite performance improvement of 30% over non-prefetching system. With one to two order of magnitude less storage and much less functional logic, SelSMaP outperforms the highest-performing prefetcher by 8.6% in CloudSuite workloads. 片上预取 T2: A Highly Accurate and Energy Efficient Stride Prefetcher. Sushant Kondguli, Michael Huang: Prefetching is a central element in most microarchitectures. Many different algorithms have been proposed with varying degrees of complexity and effectiveness. There is a tradeoff among various aspects of coverage, accuracy, and cost, especially when we try to exploit both simpler access patterns and more complex ones simultaneously. In this paper, we propose a design that only targets canonical strided access patterns, but does so with a very high accuracy. Compared to many other state-of-the-art prefetchers, sometimes with much more ambitious coverage, our design incurs much less memory traffic, reduces energy consumption, while still performs better on average. S6A: Verification and Fault Tolerance S6B: Lithography and Patterning SS1: On How to Design and Manage Complex Heterogeneous Distributed Computing Systems S7A: LCD with Focus on Emerging Technology S7B: Power-Performance Optimization of Multicore Architecture S8A: Synthesis and Security S8B: Cloud and Storage Solutions A Scale-Out Enterprise Storage Architecture. Wonil Choi, Myoungsoo Jung, Mahmut T. Kandemir, Chita R. Das: A robust enterprise SSD design should provide scalable throughput and storage capacity by integrating (up to thousands) flash chips in a scale-out fashion. However, the current \"channel-based\" SSD architecture is not a scalable design choice to allow such a dense integration. Motivated by the inherent architectural scalability of PCIe, we propose UT-SSD, a novel enterprise-scale scale-out SSD architecture, which enables the connection of a large number of (1000s) flash chips using the native PCIe buses instead of the conventional channels. We also propose an architectural enhancement that further improves the performance of our base UT-SSD by maximizing flash utilization. Our experimental analysis of UT-SSD with workloads drawn from various domains shows that the throughput of UT-SSD can reach up to 110 GB/s by successfully aggregating the bandwidth of 4096 flash chips. In addition, our proposed enhancement over this base UT-SSD increases the flash utilization by 50.7%, which in turn results in 116% additional throughput improvement. enterprise SSD CloudShelter: Protecting Virtual Machines' Memory Resource Availability in Clouds. ​ Tianwei Zhang, Yuan Xu, Yungang Bao, Ruby B. Lee: We present CloudShelter, an architecture to protect virtual machines' memory availability from undesired resource contention on the cloud servers. We introduce a new micro-architectural metric: Memory Round Trip Time, to quantify VMs' memory QoS. Using this metric, (1) CloudShelter defines new QoS options for customers when launching VMs. These options can guarantee VMs' memory QoS at different levels even when they face intensive contention with co-located VMs; (2) CloudShelter periodically monitors VMs' memory QoS at runtime: once QoS violations against customers' demands are detected, CloudShelter places this VM into an isolated environment to eliminate contention. CloudShelter can reduce 30.1% performance interference from LLC/DRAM contention and 81.6% interference from bus contention1. cloud Using Disturbance Compensation and Data Clustering (DC)2 to Improve Reliability and Performance of 3D MLC Flash Memory. Yazhi Feng, Dan Feng, Wei Tong, Yu Jiang, Chuanqi Liu 3D architectures are considered the most promising approach to continuously increasing memory density and reducing cost/bit for NAND flash memory by stacking more layers. However, 3D MLC flash memory brings two serious problems, referred to as cell-to-cell program disturbance and big block problem. To solve the disturbance problem for better reliability, we proposed a Disturbance Compensation Programming Scheme (DCPS). Based on quantitatively analyzing the disturbance from each direction in 3D flash memory, the scheme accordingly set the verify voltage (VVFY) a little lower than the original value when Incremental Step Pulse Programming (ISPP) is performed. After disturbance compensation, the threshold voltage of flash cells can shift towards the ideal distribution. Moreover, Read reference Voltage Shifting (RVS) and Artificial Compensation (AC) strategies on margin pages are introduced to adapt to the three-dimensional structures to further enhance reliability. To solve big block problem, Multiple-Level-Queue page allocation (MLQ) is proposed. We use multiple queues to filter the logical addresses of different update counts and choose different data blocks to respond. The stored data are gradually well organized and generate less data migration when performing garbage collection. Experimental results show that our design reduces the disturbed BER by at least 82% with respect to a FTL with traditional allocation and garbage collection scheme. Besides, we demonstrate MLQ can achieve more effective results than the state of the art scheme in the big block environment. The write amplification, I/O response time and the number of erasures are reduced by 31.2%, 22.2% and 14.6% on average, respectively. Read reference Voltage Shifting；减少BER Improving Performance of TLC RRAM with Compression-Ratio-Aware Data Encoding. ​ Jie Xu, Dan Feng, Wei Tong, Jingning Liu, Wen Zhou: Resistive Random Access Memory (RRAM) technology is proposed as a promising replacement candidate for DRAM-based main memory due to its good scalability, low standby power, and non-volatility. The structure of Triple-Level Cell (TLC) can offer higher data density over Single-Level Cell (SLC). However, TLC RRAM suffers from high write energy and latency. Data compression techniques can reduce the size of the data to store. In contrast, data encoding methods such as Incomplete Data Mapping (IDM) can 'expand' the size for latency and energy reduction. We observe that the compression ratio of each cacheline varies, and therefore the saved space of each compressed cacheline is different. On the other hand, we find that different IDMs have different tradeoffs in capacity and write latency/energy. To fully exploit the space saved by compression for reducing the write latency/energy, and improving the performance of TLC RRAM-based main memory system, Compression-Ratio-Aware Data Encoding (CRADE) is proposed. The key idea of CRADE is to dynamically select the best-performing IDM according to the compression ratio of each cacheline. The cacheline is compressed first, and then the compressed cacheline is encoded by IDM. For each compressed cacheline, the IDM which uses the fewest states to encode is applied on the condition that the encoded data size will not exceed the cacheline size. Experimental results show that CRADE can reduce the write energy by 15%, decrease the write latency by 19%, reduce the read latency by 4%, and improve the IPC performance by 2% compared with the state-of-the-art scheme. NVM；compression Encoding Separately: An Energy-Efficient Write Scheme for MLC STT-RAM. Jie Xu, Dan Feng, Wei Tong, Jingning Liu, Wen Zhou: Multi Level Cell (MLC) Spin Transfer Torque RAM (STT-RAM) provides higher density than Single Level Cell (SLC) STT-RAM by storing two digital bits in a single cell, and is proposed as a promising candidate for on-chip cache. However, MLC STT-RAM suffers from high write energy. We observe that general encoding methods, which map the frequent data patterns to the energy-efficient resistance states, cannot reduce the write energy of MLC STT-RAM. To reduce the write energy of MLC STT-RAM, we propose a novel encoding method, i.e., Encoding Separately (ES). The key idea of ES is to encode the hard bits and soft bits of MLCs separately. The hard bits are encoded for fewer hard-bit writes (hard transitions) and soft bits are encoded for fewer soft-bit writes (soft transitions). Specifically, existing encoding methods commonly used in SLC can be applied to MLC STT-RAM when encoding the two bits separately. We further apply two encoding methods for SLC to MLC STT-RAM through encoding separately, and experimental results show that the proposed scheme can reduce the writes to hard bits and soft bits by 28% and 16%, and achieve an energy reduction of 25%. MLC;encode Quick-and-Dirty: Improving Performance of MLC PCM by Using Temporary Short Writes. ​ Mingzhe Zhang, Lunkai Zhang, Lei Jiang, Frederic T. Chong, Zhiyong Liu: Low write performance is a major obstacle to the commercialization of MLC PCM. One opportunity for improving the latency of MLC PCM writes is to use fewer SET iterations in a single write. Unfortunately, the data written by these short writes have significantly shorter retention time and thus need frequent refreshes. As a result, it is impractical to use these short-latency, short-retention writes globally. In this paper, we analyze the temporal behavior of write operations in typical applications and propose Quick-and-Dirty (QnD), a lightweight scheme to improve the performance of MLC PCM. QnD dynamically performs the short-latency, short-retention write when write operations are bursty, and then uses short-latency, short-retention writes to mitigate the short retention problem when memory system is relatively quiet. Our experimental results show that QnD improves performance by 30.9% on geometric mean while still providing acceptable memory lifetime (7.58 years on geometric mean). We also provide sensitivity studies of the aggressiveness, memory coverage and granularity of QnD technique. PCM； SS2: Effective Voltage Scaling in Late CMOS Era SS3: Spin-Computing: Lower the Barrier between Memory and Logic S9A: Architecture and Microarchitecture Optimizations S9B: Novel Architecture with 3D and Flash Memory CooECC: A Cooperative Error Correction Scheme to Reduce LDPC Decoding Latency in NAND Flash. ​ Meng Zhang, Fei Wu, Yajuan Du, Chengmo Yang, Changsheng Xie, Jiguang Wan: The storage capacity of NAND Flash has increased by scaling down to smaller cell size and using multi-level storage technology, but data reliability is degraded by severer retention errors. To ensure data reliability, error correction codes (ECC) are adopted, such as BCH and low-density parity check (LDPC) codes. However, BCH codes are insufficient when raw bit error rates (RBER) caused by retention errors are high. As a result, BCH codes are inevitably replaced with LDPC codes with stronger error correction capability. Traditional LDPC codes are used to independently correct bit errors in the LSB and MSB pages. Unfortunately, decoding latency in such two pages is significantly unbalanced, MSB pages take much higher latency due to higher RBER, leading to suboptimal flash read performance. This paper proposes a cooperative error correction scheme, called CooECC, to reduce LDPC decoding latency of the MSB page in NAND Flash. By exploiting data error characteristics introduced by retention errors, CooECC integrates the decoding result of the LSB page into the initial information of LDPC decoding for the MSB page, making it more accurate. This in turn enables decoding to converge at a higher rate. Simulation results show that for LDPC schemes with information lengths of 2KB and 4KB, the decoding latency can be reduced by up to 87% and 84%, respectively, when RBER is as high as 8.0 × 10 -3 . LDPC Fast, Ring-Based Design of 3D Stacked DRAM. ​ Andrew J. Douglass, Sunil P. Khatri: As computer memory increases in size and processors continue to get faster, the memory subsystem becomes an increasing bottleneck to system performance. To mitigate the relatively slow DRAM memory chip speeds, a new generation of 3D stacked DRAM is being developed, with lower power consumption and higher bandwidth. This paper proposes the use of 3D ring-based data fabrics for fast data transfer between these chips. The ring-based data fabric uses a fast standing wave oscillator to clock its transactions. With a fast clocking scheme, and multiple channels sharing the same bus, more channels are utilized while significantly reducing the number of through-silicon vias (TSVs). Experimental results show that our ring-based data fabric can reduce read latencies by almost 4X compared to traditional stacked memory chips. Variations of our scheme can also reduce power consumption compared to traditional memory stacks. Our Memory Architecture using a Ring-based Scheme (MARS) can effectively trade off power, throughput, and latency to improve system performance for different application spaces. We show that our MARS variants can deliver better latency (up to ~4X), power (up to ~8X), and performance per watt (up to ~4X) over HBM, when averaged over 11 SPEC CPU 2006 benchmarks. Other MARS variants provide higher throughput with similar power consumption compared to Wide I/O memory. 3D Stacked DRAM Memory-Bounded Randomness for Hardware-Constrained Encrypted Computation. ​ Nektarios Georgios Tsoutsos, Oleg Mazonka, Michail Maniatakos: Encrypted computation enables processing sensitive data directly in the encrypted domain, which allows outsourcing to third parties without compromising privacy. Recent solutions that leverage partial homomorphic encryption, however, require excessive lookup tables or obfuscated software oracles to implement branching over encrypted control values. To address these limitations and make encrypted computations more practical on memory-constrained systems, we present a novel approach for limiting the amount of randomness in probabilistic ciphertexts, using number theory primitives and hash tables. This allows de-randomizing probabilistic ciphertexts and define a new encrypted abstract machine that is memory-friendly to the target system. Compared to obfuscated oracles in previous work, our method performs control flow decisions over ciphertexts twice as fast, while requiring selectively small lookup tables. Exploiting Process Variation for Read Performance Improvement on LDPC Based Flash Memory Storage Systems. ​ Qiao Li, Liang Shi, Yejia Di, Yajuan Du, Chun Jason Xue, Edwin Hsing-Mean Sha: With the development of bit density and technology scaling, the process variation (PV) has become much severe on NAND flash memory. As PV presents reliability among flash blocks, which causes read performance variation to read data on different blocks. This paper proposes to improve read performance of LDPC based flash memory by exploiting the reliability characteristics of PV. First, a block grouping approach is proposed to classify the flash blocks based on their reliability. Then, a read data placement scheme is proposed, which is designed to place read-hot data on flash blocks with high reliability and move read-cold data to blocks with low reliability. Experiment results show that, with negligible overhead, the proposed scheme is able to significantly improve the read performance. A Design-for-Test Solution for Monolithic 3D Integrated Circuits. ​ Abhishek Koneru, Sukeshwar Kannan, Krishnendu Chakrabarty Monolithic three-dimensional (M3D) integration has the potential to achieve significantly higher device density compared to 3D integration based on through-silicon vias (TSVs). We propose a test solution for M3D ICs based on dedicated test layers that are inserted between functional layers. We evaluate the cost associated with the proposed design-for-test (DfT) solution and compare it with that for a potential DfT solution based on the IEEE Std. P1838. Our results show that the proposed solution is more cost-efficient than the P1838-based solution for a wide range of inter-layer via (ILV) density, ILV yield, and defect density. Copyright © jabingu.com 2019 all right reserved，powered by GitbookFile modification time： 2019-08-20 14:55:48 "},"conference/4-IPDPS-2017.html":{"url":"conference/4-IPDPS-2017.html","title":"IPDPS 2017","keywords":"","body":"IPDPS 2017 死亡是活过的生命，生活是在路上的死亡。 ——博尔赫斯　　 目录： S3: Caches S27: Compression & Memoization S28: Persistent Memory S3: Caches Elastic-Cache: GPU Cache Architecture for Efficient Fine- and Coarse-Grained Cache-Line Management. ​ Bingchao Li, Jizhou Sun, Murali Annavaram, Nam Sung Kim: GPUs provide high-bandwidth/low-latency on-chip shared memory and L1 cache to efficiently service a large number of concurrent memory requests (to contiguous memory space). To support warp-wide accesses to L1 cache, GPU L1 cache lines are very wide. However, such L1 cache architecture cannot always be efficiently utilized when applications generate many memory requests with irregular access patterns especially due to branch and memory divergences. In this paper, we propose Elastic-Cache that can efficiently support both fine- and coarse-grained L1 cache-line management for applications with both regular and irregular memory access patterns. Specifically, it can store 32- or 64-byte words in non-contiguous memory space to a single 128-byte cache line. Furthermore, it neither requires an extra tag storage structure nor reduces the capacity of L1 cache since it stores auxiliary tags for fine-grained L1 cache-line managements in sharedmemory space that is not fully used in many applications. Our experiment shows that Elastic-Cache improves the geo-mean performance of applications with irregular memory access patterns by 58% without degrading performance of applications with regular memory access patterns. Content-Aware Non-Volatile Cache Replacement. ​ Qi Zeng, Jih-Kwon Peir: Spin-Transfer Torque Magnetoresistive Random-Access Memory (STT-MRAM) is a promising memory technology, which has high density, fast read speed, low leakage power, and non-volatility, and is suitable for multi-core on-chip last-level caches. However, the high write energy and latency, as well as less-than-desirable write endurance of STT-MRAM remain challenges. This paper proposes a new encoded content-aware cache replacement policy to reduce the total switch bits for write, lower the write energy, and improve write endurance. Instead of replacing the LRU block under the conventional pseudo-LRU replacement policy, we select a replacement block near the LRU position, which has the most similar content to the missed block. The selected replacement block can reduce the switch bits without damaging the cache performance. To avoid fetching and comparing the entire block contents, we present a novel content encoding method to encode 64-byte block using just 8 bits, each bit represents 8-byte content. The encoded bit is determined by the presence of a dominant bit value in the 8 bytes. We measure the content similarity using the Hamming distance between the encoded bits of the missed block and the replaced block. Performance evaluation demonstrates that the proposed simple content encoding method is effective with an average of 20.5% reduction in total switch bits, which results in improvement on write endurance and less write energy consumption. These improvements are accomplished with low overhead and minimum impact on the cache performance. DEFT-Cache: A Cost-Effective and Highly Reliable SSD Cache for RAID Storage. ​ Jiguang Wan, Wei Wu, Ling Zhan, Qing Yang, Xiaoyang Qu, Changsheng Xie: This paper proposes a new SSD cache architecture, DEFT-cache, Delayed Erasing and Fast Taping, that maximizes I/O performance and reliability of RAID storage. First of all, DEFT-Cache exploits the inherent physical properties of flash memory SSD by making use of old data that have been overwritten but still in existence in SSD to minimize small write penalty of RAID5/6. As data pages being overwritten in SSD, old data pages are invalidated and become candidates for erasure and garbage collections. Our idea is to selectively delay the erasure of the pages and let these otherwise useless old data in SSD contribute to I/O performance for parity computations upon write I/Os. Secondly, DEFT-Cache provides inexpensive redundancy to the SSD cache by having one physical SSD and one virtual SSD as a mirror cache. The virtual SSD is implemented on HDD but using log-structured data layout, i.e. write data are quickly logged to HDD using sequential write. The dual and redundant caches provide a cost-effective and highly reliable write-back SSD cache. We have implemented DEFT-Cache on Linux system. Extensive experiments have been carried out to evaluate the potential benefits of our new techniques. Experimental results on SPC and Microsoft traces have shown that DEFT-Cache improves I/O performance by 26.81% to 56.26% in terms of average user response time. The virtual SSD mirror cache can absorb write I/Os as fast as physical SSD providing the same reliability as two physical SSD caches without noticeable performance loss. Adaptive Software Caching for Efficient NVRAM Data Persistence. ​ Pengcheng Li, Dhruva R. Chakrabarti, Chen Ding, Liang Yuan: Non-volatile main memory (NVRAM) enables data persistence in memory. However, the existence of transient CPU caches in modern computer architectures brings a serious performance issue. In particular, cache lines have to be flushed frequently to guarantee consistent persistent program states. Hence, persistence and performance cannot be easily obtained simultaneously. In this paper, we optimize data persistence by proposing a software cache. The software cache first buffers lines that need to be flushed, and then flushes them out at an appropriate later time. The software cache aims to maximize the combination of cache line flushes. We designed a new linear-time algorithm to calculate cache miss ratio curve (MRC) so as to adaptively select the best cache capacity at run-time based on program behavior. We evaluated the software cache on a real-world memory-based database benchmark, the SPLASH2 benchmark suite and four micro-benchmarks. Results indicate that the software cache solution reduces cache write backs to persistent memory by 12× and improves performance over the state-of- the-art methods by 2.1× on average, measured on a real system emulator. S27: Compression & Memoization Elastic Data Compression with Improved Performance and Space Efficiency for Flash-Based Storage Systems. ​ Bo Mao, Hong Jiang, Suzhen Wu, Yaodong Yang, Zaifa Xi: Data compression has become a commodity feature for space efficiency and reliability in flash-based storage systems by reducing write traffic and space capacity demand. However, it introduces noticeable processing overheads on the critical I/O path, which degrades the system performance significantly. Existing data compression schemes for flash-based storage systems use fixed compression algorithms for all the incoming write data, failing to recognize and exploit the significant diversity in compressibility and access patterns of data and missing an opportunity to improve the system performance, the space efficiency or both. To achieve a reasonable trade-off between these two important design objectives, in this paper we introduce an Elastic Data Compression scheme, called EDC, which exploits the data compressibility and access intensity characteristics by judiciously matching data of different compressibility with different compression algorithms while leveraging the access idleness. Specifically, for compressible data blocks EDC exploits the compression diversity of the workload, and employs algorithms of higher compression rate in periods of lower system utilization and algorithms of lower compression rate in periods of higher system utilization. For non-compressible (or very lowly compressible) data blocks, it will write them through to the flash storage directly without any compression. The experiments conducted on our lightweight prototype implementation of the EDC system show that EDC saves storage space by up to 38.7%, with an average of 33.7%. In addition, it significantly outperforms the fixed compression schemes in the I/O performance measure by up to 61.4%, with an average of 36.7%. E^2MC: Entropy Encoding Based Memory Compression for GPUs. Sohan Lal, Jan Lucas, Ben H. H. Juurlink: Modern Graphics Processing Units (GPUs) provide much higher off-chip memory bandwidth than CPUs, but many GPU applications are still limited by memory bandwidth. Unfortunately, off-chip memory bandwidth is growing slower than the number of cores and has become a performance bottleneck. Thus, optimizations of effective memory bandwidth play a significant role for scaling the performance of GPUs. Memory compression is a promising approach for improving memory bandwidth which can translate into higher performance and energy efficiency. However, compression is not free and its challenges need to be addressed, otherwise the benefits of compression may be offset by its overhead. We propose an entropy encoding based memory compression (E2MC) technique for GPUs, which is based on the well-known Huffman encoding. We study the feasibility of entropy encoding for GPUs and show that it achieves higher compression ratios than state-of-the-art GPU compression techniques. Furthermore, we address the key challenges of probability estimation, choosing an appropriate symbol length for encoding, and decompression with low latency. The average compression ratio of E2MC is 53% higher than the state of the art. This translates into an average speedup of 20% compared to no compression and 8% higher compared to the state of the art. Energy consumption and energy-delayproduct are reduced by 13% and 27%, respectively. Moreover, the compression ratio achieved by E2MC is close to the optimal compression ratio given by Shannon's source coding theorem. Significantly Improving Lossy Compression for Scientific Data Sets Based on Multidimensional Prediction and Error-Controlled Quantization. Dingwen Tao, Sheng Di, Zizhong Chen, Franck Cappello: Today's HPC applications are producing extremely large amounts of data, such that data storage and analysis are becoming more challenging for scientific research. In this work, we design a new error-controlled lossy compression algorithm for large-scale scientific data. Our key contribution is significantly improving the prediction hitting rate (or prediction accuracy) for each data point based on its nearby data values along multiple dimensions. We derive a series of multilayer prediction formulas and their unified formula in the context of data compression. One serious challenge is that the data prediction has to be performed based on the preceding decompressed values during the compression in order to guarantee the error bounds, which may degrade the prediction accuracy in turn. We explore the best layer for the prediction by considering the impact of compression errors on the prediction accuracy. Moreover, we propose an adaptive error-controlled quantization encoder, which can further improve the prediction hitting rate considerably. The data size can be reduced significantly after performing the variable-length encoding because of the uneven distribution produced by our quantization encoder. We evaluate the new compressor on production scientific data sets and compare it with many other state-of-the-art compressors: GZIP, FPZIP, ZFP, SZ-1.1, and ISABELA. Experiments show that our compressor is the best in class, especially with regard to compression factors (or bit-rates) and compression errors (including RMSE, NRMSE, and PSNR). Our solution is better than the second-best solution by more than a 2x increase in the compression factor and 3.8x reduction in the normalized root mean squared error on average, with reasonable error bounds and user-desired bit-rates. ATM: Approximate Task Memoization in the Runtime System. ​ Iulian Brumar, Marc Casas, Miquel Moretó, Mateo Valero, Gurindar S. Sohi: Redundant computations appear during the execution of real programs. Multiple factors contribute to these unnecessary computations, such as repetitive inputs and patterns, calling functions with the same parameters or bad programming habits. Compilers minimize non useful code with static analysis. However, redundant execution might be dynamic and there are no current approaches to reduce these inefficiencies. Additionally, many algorithms can be computed with different levels of accuracy. Approximate computing exploits this fact to reduce execution time at the cost of slightly less accurate results. In this case, expert developers determine the desired tradeoff between performance and accuracy for each application. In this paper, we present Approximate Task Memoization (ATM), a novel approach in the runtime system that transparently exploits both dynamic redundancy and approximation at the task granularity of a parallel application. Memoization of previous task executions allows predicting the results of future tasks without having to execute them and without losing accuracy. To further increase performance improvements, the runtime system can memoize similar tasks, which leads to task approximate computing. By defining how to measure task similarity and correctness, we present an adaptive algorithm in the runtime system that automatically decides if task approximation is beneficial or not. When evaluated on a real 8-core processor with applications from different domains (financial analysis, stencil-computation, machine-learning and linear-algebra), ATM achieves a 1.4x average speedup when only applying memoization techniques. When adding task approximation, ATM achieves a 2.5x average speedup with an average 0.7% accuracy loss (maximum of 3.2%). S28: Persistent Memory Design and Implementation of Papyrus: Parallel Aggregate Persistent Storage. ​ Jungwon Kim, Kittisak Sajjapongse, Seyong Lee, Jeffrey S. Vetter: A surprising development in recently announced HPC platforms is the addition of, sometimes massive amounts of, persistent (nonvolatile) memory (NVM) in order to increase memory capacity and compensate for plateauing I/O capabilities. However, there are no portable and scalable programming interfaces using aggregate NVM effectively. This paper introduces Papyrus: a new software system built to exploit emerging capability of NVM in HPC architectures. Papyrus (or Parallel Aggregate Persistent -YRU- Storage) is a novel programming system that provides features for scalable, aggregate, persistent memory in an extreme-scale system for typical HPC usage scenarios. Papyrus mainly consists of Papyrus Virtual File System (VFS) and Papyrus Template Container Library (TCL). Papyrus VFS provides a uniform aggregate NVM storage image across diverse NVM architectures. It enables Papyrus TCL to provide a portable and scalable high-level container programming interface whose data elements are distributed across multiple NVM nodes without requiring the user to handle complex communication, synchronization, replication, and consistency model. We evaluate Papyrus on two HPC systems, including UTK Beacon and NERSC Cori, using real NVM storage devices. Language-Based Optimizations for Persistence on Nonvolatile Main Memory Systems. Joel Edward Denny, Seyong Lee, Jeffrey S. Vetter: Substantial advances in nonvolatile memory (NVM) technologies have motivated wide-spread integration of NVM into mobile, enterprise, and HPC systems. Recently, considerable research has focused on architectural integration of NVM and respective programming systems, exploiting NVM's trait of persistence correctly and efficiently. In this regard, we design several novel language-based optimization techniques for programming NVM and demonstrate them as an extension of our NVL-C system. Specifically, we focus on optimizing the performance of atomic updates to complex data structures residing in NVM. We build on two variants of automatic undo logging: canonical undo logging, and shadow updates. We show these techniques can be implemented transparently and efficiently, using dynamic selection and other logging optimizations. Our empirical results on several applications gathered on an NVM testbed illustrate that our cost-model-based dynamic selection technique can accurately choose the best logging variant across different NVM modes and input sizes. In comparison to statically choosing canonical undo logging, this improvement reduces execution time to as little as 53% for block-addressable NVM and 73% for emulated byte-addressable NVM on a Fusion-io ioScale device. MetaKV: A Key-Value Store for Metadata Management of Distributed Burst Buffers. ​ Teng Wang, Adam Moody, Yue Zhu, Kathryn Mohror, Kento Sato, Tanzima Islam, Weikuan Yu: Distributed burst buffers are a promising storage architecture for handling I/O workloads for exascale computing. Their aggregate storage bandwidth grows linearly with system node count. However, although scientific applications can achieve scalable write bandwidth by having each process write to its node-local burst buffer, metadata challenges remain formidable, especially for files shared across many processes. This is due to the need to track and organize file segments across the distributed burst buffers in a global index. Because this global index can be accessed concurrently by thousands or more processes in a scientific application, the scalability of metadata management is a severe performance-limiting factor. In this paper, we propose MetaKV: a key-value store that provides fast and scalable metadata management for HPC metadata workloads on distributed burst buffers. MetaKV complements the functionality of an existing key-value store with specialized metadata services that efficiently handle bursty and concurrent metadata workloads: compressed storage management, supervised block clustering, and log-ring based collective message reduction. Our experiments demonstrate that MetaKV outperforms the state-of-the-art key-value stores by a significant margin. It improves put and get metadata operations by as much as 2.66× and 6.29×, respectively, and the benefits of MetaKV increase with increasing metadata workload demand. Parallelism and Garbage Collection Aware I/O Scheduler with Improved SSD Performance. ​ Jiayang Guo, Yiming Hu, Bo Mao, Suzhen Wu: In this paper, we propose PGIS, a parallelism and garbage collection aware I/O Scheduler, which identifies the hot data based on trace characteristics to exploit the channel level internal parallelism of flash-based storage systems. PGIS not only fully exploits abundant channel resource in the SSD, but also it introduces a hot data identification mechanism to reduce the garbage collection overhead. By dispatching hot read data to different channel, the channel level internal parallelism is fully exploited. By dispatching hot write data to the same physical block, the garbage collection overhead has been alleviated. The experiment results show that compared with existing I/O schedulers, PGIS improves the response time and garbage collection performance significantly. Consequently, PGIS reduces the garbage collection overhead up to 30.9%, while exploiting channel level internal parallelism. Copyright © jabingu.com 2019 all right reserved，powered by GitbookFile modification time： 2019-08-20 14:55:55 "},"conference/5-EuroSys-2019.html":{"url":"conference/5-EuroSys-2019.html","title":"EuroSys 2019","keywords":"","body":"EuroSys 2019 死亡是活过的生命，生活是在路上的死亡。 ——博尔赫斯　　 目录： SecurityExploiting CPU ArchitectureOS KernelStorage SystemsDatacenter SystemsNetworkingBig DataDistributed SystemsCloud ComputingProgramming Languages and VerificationSystems for Machine Learning Security Exploiting CPU Architecture OS Kernel Storage Systems Project Almanac: A Time-Traveling Solid-State Drive. Xiaohao Wang, Yifan Yuan, You Zhou, Chance C. Coats, Jian Huang: Systems and Platform Research Group, Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign Preserving the history of storage states is critical to ensuring system reliability and security. It facilitates system functions such as debugging, data recovery, and forensics. Existing software-based approaches like data journaling, logging, and backups not only introduce performance and storage cost, but also are vulnerable to malware attacks, as adversaries can obtain kernel privileges to terminate or destroy them. In this paper, we present Project Almanac, which includes (1) a time-travel solid-state drive (SSD) named TimeSSD that retains a history of storage states in hardware for a window of time, and (2) a toolkit named TimeKits that provides storage-state query and rollback functions. TimeSSD tracks the history of storage states in the hardware device, without relying on explicit backups, by exploiting the property that the flash retains old copies of data when they are updated or deleted. We implement TimeSSD with a programmable SSD and develop TimeKits for several typical system applications. Experiments, with a variety of real-world case studies, demonstrate that TimeSSD can retain all the storage states for eight weeks, with negligible performance overhead, while providing the device-level time-travel property. ShieldStore: Shielded In-memory Key-value Storage with SGX Taehoon Kim, Joongun Park, Jaewook Woo, Seungheun Jeon, Jaehyuk Huh School of Computing, KAIST The shielded computation of hardware-based trusted execution environments such as Intel Software Guard Extensions (SGX) can provide secure cloud computing on remote systems under untrusted privileged system software. However, hardware overheads for securing protected memory restrict its capacity to a modest size of several tens of megabytes, and more demands for protected memory beyond the limit cause costly demand paging. Although one of the widely used applications benefiting from the enhanced security of SGX, is the in-memory key-value store, its memory requirements are far larger than the protected memory limit. Furthermore, the main data structures commonly use fine-grained data items such as pointers and keys, which do not match well with the coarse-grained paging of the SGX memory extension technique. To overcome the memory restriction, this paper proposes a new in-memory key-value store designed for SGX with application-specific data security management. The proposed key-value store, called ShieldStore, maintains the main data structures in unprotected memory with each key-value pair individually encrypted and integrity-protected by its secure component running inside an enclave. Based on the enclave protection by SGX, ShieldStore provides secure data operations much more efficiently than the baseline SGX key-value store, achieving 8--11 times higher throughput with 1 thread, and 24--30 times higher throughput with 4 threads. In-memory；索引 URSA: Hybrid Block Storage for Cloud-Scale Virtual Disks. Huiba Li Alibaba, Beijing, China Yiming Zhang NiceX Lab, PDL, NUDT, Changsha, Hunan, China Dongsheng Li NUDT, Changsha, Hunan, China Zhiming Zhang mos.meituan.com, Beijing, China Shengyun Liu NUDT, Changsha, Hunan, China Peng Huang Johns Hopkins University, Baltimore, Maryland, USA Zheng Qin NUDT, Changsha, Hunan, China Kai Chen HKUST, Hong Kong, China Yongqiang Xiong Microsoft, Beijing, China This paper presents URSA, a hybrid block store that provides virtual disks for various applications to run efficiently on cloud VMs. Trace analysis shows that the I/O patterns served by block storage have limited locality to exploit. Therefore, instead of using SSDs as a cache layer, URSA proposes an SSD-HDD-hybrid storage structure that directly stores primary replicas on SSDs and replicates backup replicas on HDDs, using journals to bridge the performance gap between SSDs and HDDs. URSA integrates the hybrid structure with designs for high reliability, scalability, and availability. Experiments show that URSA in its hybrid mode achieves almost the same performance as in its SSD-only mode (storing all replicas on SSDs), and outperforms other block stores (Ceph and Sheepdog) even in their SSD-only mode while achieving much higher CPU efficiency (performance per core). We also discuss some practical issues in our deployment. VStore: A Data Store for Analytics on Large Videos Tiantu Xu, Luis Materon Botelho, Felix Xiaozhu Lin Purdue ECE We present VStore, a data store for supporting fast, resource-efficient analytics over large archival videos. VStore manages video ingestion, storage, retrieval, and consumption. It controls video formats along the video data path. It is challenged by i) the huge combinatorial space of video format knobs; ii) the complex impacts of these knobs and their high profiling cost; iii) optimizing for multiple resource types. It explores an idea called backward derivation of configuration: in the opposite direction along the video data path, VStore passes the video quantity and quality expected by analytics backward to retrieval, to storage, and to ingestion. In this process, VStore derives an optimal set of video formats, optimizing for different resources in a progressive manner. VStore automatically derives large, complex configurations consisting of more than one hundred knobs over tens of video formats. In response to queries, VStore selects video formats catering to the executed operators and the target accuracy. It streams video data from disks through decoder to operators. It runs queries as fast as 362x of video realtime. Datacenter Systems Managing Tail Latency in Datacenter-Scale File Systems Under Production Constraints. Pulkit A. Misra Duke University María F. Borge University of Sydney Íñigo Goiri Microsoft Research Alvin R. Lebeck Duke University Willy Zwaenepoel University of Sydney and EPFL Ricardo Bianchini Microsoft Research Distributed file systems often exhibit high tail latencies, especially in large-scale datacenters and in the presence of competing (and possibly higher priority) workloads. This paper introduces techniques for managing tail latencies in these systems, while addressing the practical challenges inherent in production datacenters (e.g., hardware heterogeneity, interference from other workloads, the need to maximize simplicity and maintainability). We implement our techniques in a scalable distributed file system (an extension of HDFS) used in production at Microsoft. Our evaluation uses 70k servers in 3 datacenters, and shows that our techniques reduce tail latency significantly for production workloads. Wormhole: A Fast Ordered Index for In-memory Data Management Xingbo Wu University of Illinois at Chicago Fan Ni University of Texas at Arlington Song Jiang University of Texas at Arlington In-memory data management systems, such as key-value stores, have become an essential infrastructure in today's big-data processing and cloud computing. They rely on efficient index structures to access data. While unordered indexes, such as hash tables, can perform point search with O(1) time, they cannot be used in many scenarios where range queries must be supported. Many ordered indexes, such as B+ tree and skip list, have a O(log N) lookup cost, where N is number of keys in an index. For an ordered index hosting billions of keys, it may take more than 30 key-comparisons in a lookup, which is an order of magnitude more expensive than that on a hash table. With availability of large memory and fast network in today's data centers, this O(log N) time is taking a heavy toll on applications that rely on ordered indexes. In this paper we introduce a new ordered index structure, named Wormhole, that takes O(log L) worst-case time for looking up a key with a length of L. The low cost is achieved by simultaneously leveraging strengths of three indexing structures, namely hash table, prefix tree, and B+ tree, to orchestrate a single fast ordered index. Wormhole's range operations can be performed by a linear scan of a list after an initial lookup. This improvement of access efficiency does not come at a price of compromised space efficiency. Instead, Wormhole's index space is comparable to those of B+ tree and skip list. Experiment results show that Wormhole outperforms skip list, B+ tree, ART, and Masstree by up to 8.4x, 4.9x, 4.3x, and 6.6x in terms of key lookup throughput, respectively. in-memory；索引 Scalable RDMA RPC on Reliable Connection with Efficient Resource Sharing. Youmin Chen, Youyou Lu, Jiwu Shu Tsinghua University RDMA provides extremely low latency and high bandwidth to distributed systems. Unfortunately, it fails to scale and suffers from performance degradation when transferring data to an increasing number of targets on Reliable Connection (RC). We observe that the above scalability issue has its root in the resource contention in the NIC cache, the CPU cache and the memory of each server. In this paper, we propose ScaleRPC, an efficient RPC primitive using one-sided RDMA verbs on reliable connection to provide scalable performance. To effectively alleviate the resource contention, ScaleRPC introduces 1) connection grouping to organize the network connections into groups, so as to balance the saturation and thrashing of the NIC cache; 2) virtualized mapping to enable a single message pool to be shared by different groups of connections, which reduces CPU cache misses and improve memory utilization. Such scalable connection management provides substantial performance benefits: By deploying ScaleRPC both in a distributed file system and a distributed transactional system, we observe that it achieves high scalability and respectively improves performance by up to 90% and 160% for metadata accessing and SmallBank transaction processing. FlyMC: Highly Scalable Testing of Complex Interleavings in Distributed Systems Jeffrey F. Lukman University of Chicago Huan Ke University of Chicago Cesar A. Stuardo University of Chicago Riza O. Suminto University of Chicago Daniar H. Kurniawan University of Chicago Dikaimin Simon Surya University Satria Priambada Bandung Institute of Technology Chen Tian Huawei US R&D Center Feng Ye Huawei US R&D Center Tanakorn Leesatapornwongsa Samsung Research America Aarti Gupta Princeton University Shan Lu University of Chicago Haryadi S. Gunawi University of Chicago We present a fast and scalable testing approach for datacenter/cloud systems such as Cassandra, Hadoop, Spark, and ZooKeeper. The uniqueness of our approach is in its ability to overcome the path/state-space explosion problem in testing workloads with complex interleavings of messages and faults. We introduce three powerful algorithms: state symmetry, event independence, and parallel flips, which collectively makes our approach on average 16x (up to 78x) faster than other state-of-the-art solutions. We have integrated our techniques with 8 popular datacenter systems, successfully reproduced 12 old bugs, and found 10 new bugs --- all were done without random walks or manual checkpoints. Networking Big Data Distributed Systems Cloud Computing Resource Deflation: A New Approach For Transient Resource Reclamation Prateek Sharma Indiana University Ahmed Ali-Eldin University of Massachusetts Amherst Prashant Shenoy University of Massachusetts Amherst Data centers and clouds are increasingly offering low-cost computational resources in the form of transient virtual machines. Whenever demand for computational resources exceeds their availability, transient resources can reclaimed by preempting the transient VMs. Conventionally, these transient VMs are used by low-priority applications that can tolerate the disruption caused by preemptions. In this paper we propose an alternative approach for reclaiming resources, called resource deflation. Resource deflation allows applications to dynamically shrink (and expand) in response to resource pressure, instead of being preempted outright. Deflatable VMs allow applications to continue running even under resource pressure, and increase the utility of low-priority transient resources. Deflation uses a dynamic, multi-level cascading reclamation technique that allows applications, operating systems, and hypervisors to implement their own policies for handling resource pressure. For distributed data processing, machine learning, and deep neural network training, our multi-level approach reduces the performance degradation by up to 2x compared to existing preemption-based approaches. When deflatable VMs are deployed on a cluster, our policies allow up to 1.6x utilization without the risk of preemption. GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks Ram Srivatsa Kannan University of Michigan, Ann Arbor Lavanya Subramanian Facebook and Intel Labs Ashwin Raju University of Texas at Arlington Jeongseob Ahn Ajou University Jason Mars University of Michigan, Ann Arbor Lingjia Tang University of Michigan, Ann Arbor The microservice architecture has dramatically reduced user effort in adopting and maintaining servers by providing a catalog of functions as services that can be used as building blocks to construct applications. This has enabled datacenter operators to look at managing datacenter hosting microservices quite differently from traditional infrastructures. Such a paradigm shift calls for a need to rethink resource management strategies employed in such execution environments. We observe that the visibility enabled by a microservices execution framework can be exploited to achieve high throughput and resource utilization while still meeting Service Level Agreements, especially in multi-tenant execution scenarios. In this study, we present GrandSLAm, a microservice execution framework that improves utilization of datacenters hosting microservices. GrandSLAm estimates time of completion of requests propagating through individual microservice stages within an application. It then leverages this estimate to drive a runtime system that dynamically batches and reorders requests at each microservice in a manner where individual jobs meet their respective target latency while achieving high throughput. GrandSLAm significantly increases throughput by up to 3x compared to the our baseline, without violating SLAs for a wide range of real-world AI and ML applications. Hourglass: Leveraging Transient Resources for Time-Constrained Graph Processing in the Cloud Pedro Joaquim, Manuel Bravo, Luís E. T. Rodrigues, Miguel Matos INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal This paper addresses the key problems that emerge when one attempts to use transient resources to reduce the cost of running time-constrained jobs in the cloud. Previous works fail to address these problems and are either not able to offer significant savings or miss termination deadlines. First, the fact that transient resources can be evicted, requiring the job to be re-started (even if not from scratch) may lead provisioning policies to fall-back to expensive on-demand configurations more often than desirable, or even to miss deadlines. Second, when a job is restarted, the new configuration can be different from the previous, which might make eviction recovery costly, e.g., transferring the state of graph data between the old and new configurations. We present HOURGLASS, a system that addresses these issues by combining two novel techniques: a slack-aware provisioning strategy that selects configurations considering the remaining time before the job's termination deadline, and a fast reload mechanism to quickly recover from evictions. By switching to an on-demand configuration when (but only if) the target deadline is at risk of not being met, we are able to obtain significant cost savings while always meeting the deadlines. Our results show that, unlike previous work, HOURGLASS is able to significantly reduce the operating costs in the order of 60-70% while guaranteeing that deadlines are met. 云环境下的任务调度 Efficient, Consistent Distributed Computation with Predictive Treaties Tom Magrino Cornell University, Ithaca, NY, USA Jed Liu Barefoot Networks Ithaca, NY, USA Nate Foster Cornell University, Ithaca, NY, USA Johannes Gehrke Microsoft Corporation Redmond, WA, USA Andrew C. Myers Cornell University, Ithaca, NY, USA To achieve good performance, modern applications often partition their state across multiple geographically distributed nodes. While this approach reduces latency in the common case, it can be challenging for programmers to use correctly, especially in applications that require strong consistency. We introduce predictive treaties, a mechanism that can significantly reduce distributed coordination without losing strong consistency. The central insight behind our approach is that many computations can be expressed in terms of predicates over distributed state that can be partitioned and enforced locally. Predictive treaties improve on previous work by allowing the locally enforced predicates to depend on time. Intuitively, by predicting the evolution of system state, coordination can be significantly reduced compared to static approaches. We implemented predictive treaties in a distributed system that exposes them in an intuitive programming model. We evaluate performance on several benchmarks, including TPC-C, showing that predictive treaties can significantly increase performance by orders of magnitude and can even outperform customized algorithms. Programming Languages and Verification Systems for Machine Learning Copyright © jabingu.com 2019 all right reserved，powered by GitbookFile modification time： 2019-08-20 14:56:01 "}}